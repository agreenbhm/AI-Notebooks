{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agreenbhm/AI-Notebooks/blob/main/Copy_of_Colab_TextGen_Alpaca_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Install the web UI\n",
        "\n",
        "save_logs_to_google_drive = False #@param {type:\"boolean\"}\n",
        "Model = \"GPT4-X-Alpaca-13B\" #@param [\"GPT4-X-Alpaca-13B\"]\n",
        "\n",
        "if save_logs_to_google_drive:\n",
        "  import os\n",
        "  import shutil\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  base_folder = '/content/drive/MyDrive'\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/oobabooga/text-generation-webui\n",
        "if save_logs_to_google_drive:\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data\"):\n",
        "    os.mkdir(f\"{base_folder}/oobabooga-data\")\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data/logs\"):\n",
        "    os.mkdir(f\"{base_folder}/oobabooga-data/logs\")\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data/softprompts\"):\n",
        "    os.mkdir(f\"{base_folder}/oobabooga-data/softprompts\")\n",
        "  if not os.path.exists(f\"{base_folder}/oobabooga-data/characters\"):\n",
        "    shutil.move(\"text-generation-webui/characters\", f\"{base_folder}/oobabooga-data/characters\")\n",
        "  else:\n",
        "    !rm -r \"text-generation-webui/characters\"\n",
        "    \n",
        "  !rm -r \"text-generation-webui/softprompts\"\n",
        "  !ln -s \"$base_folder/oobabooga-data/logs\" \"text-generation-webui/logs\"\n",
        "  !ln -s \"$base_folder/oobabooga-data/softprompts\" \"text-generation-webui/softprompts\"\n",
        "  !ln -s \"$base_folder/oobabooga-data/characters\" \"text-generation-webui/characters\"\n",
        "\n",
        "else:\n",
        "  !mkdir text-generation-webui/logs\n",
        "\n",
        "!ln -s text-generation-webui/logs .\n",
        "!ln -s text-generation-webui/characters .\n",
        "!ln -s text-generation-webui/models .\n",
        "%rm -r sample_data\n",
        "%cd text-generation-webui\n",
        "!wget https://oobabooga.github.io/settings-colab.json -O settings-colab-template.json\n",
        "\n",
        "# Install requirements\n",
        "!pip install -r requirements.txt\n",
        "!pip install -r extensions/google_translate/requirements.txt\n",
        "!pip install -r extensions/silero_tts/requirements.txt\n",
        "print(f\"\\033[1;32;1m\\n --> If you see a warning about \\\"pydevd_plugins\\\", just ignore it and move on to Step 3. There is no need to restart the runtime.\\n\\033[0;37;0m\")\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!apt install ./cloudflared-linux-amd64.deb aria2\n",
        "!rm cloudflared-linux-amd64.deb\n",
        "\n",
        "if Model == \"GPT4-X-Alpaca-13B\":\n",
        "  # Download LLaMA-13B model using aria2c\n",
        "  !mkdir repositories ; cd repositories ; git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda ; cd GPTQ-for-LLaMa ; python setup_cuda.py install\n",
        "  !export GIT_LFS_SKIP_SMUDGE=1 ; cd models ; git clone https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g ; cd gpt4-x-alpaca-13b-native-4bit-128g; rm gpt-x-alpaca-13b-native-4bit-128g-cuda.pt gpt-x-alpaca-13b-native-4bit-128g.pt training_args.bin tokenizer.model gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin ; wget https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/resolve/main/training_args.bin ; wget https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/resolve/main/tokenizer.model ; aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g/resolve/main/gpt-x-alpaca-13b-native-4bit-128g-cuda.pt -o gpt-x-alpaca-13b-native-4bit-128g-cuda.pt\n"
      ],
      "metadata": {
        "id": "LGQ8BiMuXMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Launch\n",
        "\n",
        "!pkill cloudflared\n",
        "!rm nohup.out 2> /dev/null\n",
        "!nohup cloudflared --url http://127.0.0.1:7860 &\n",
        "!sleep 5 && grep --color=never -o 'https://.*trycloudflare.com' < nohup.out | sed 's/^/Once the local URL below shows, the Cloudflare URL is: /'\n",
        "\n",
        "if Model == \"GPT4-X-Alpaca-13B\":\n",
        "  # Launch the model with specific parameters\n",
        "  !python server.py --model gpt4-x-alpaca-13b-native-4bit-128g --wbits 4 --groupsize 128"
      ],
      "metadata": {
        "id": "hKuocueuXnm5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}